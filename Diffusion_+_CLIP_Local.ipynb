{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diffusion + CLIP Local.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MSFTserver/AI-Colab-Notebooks-Local/blob/main/Diffusion_%2B_CLIP_Local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**INSTRUCTIONS:**\n",
        "\n",
        "###1) Press Play Button on all Code Block to download everything\n",
        " - includes 512 and 256 versions\n",
        " \n",
        "###2) Enter Text and Number of Images you want to generate\n",
        " - all other options i made available you can mess with them or not \n",
        "\n",
        "###3) Run The Art Generator! \n",
        "\n",
        "**Edited by [HostsServer](https://github.com/MSFTserver) for local runtime use!**"
      ],
      "metadata": {
        "id": "SGJahek5Xoqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1)** install Dependcies and get models ready\n",
        " - do not run all, run them 1 by please"
      ],
      "metadata": {
        "id": "TOh7l8-p51qP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ3rNuAWAewx",
        "cellView": "form"
      },
      "source": [
        "#@markdown #**Check GPU type**\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown 4Gb = Poor\n",
        "\n",
        "#@markdown 6GB = Average\n",
        "\n",
        "#@markdown 8GB = Good\n",
        "\n",
        "#@markdown 10 GB = Great\n",
        "\n",
        "#@markdown 12+GB = MUZZZ BE NICEEE.....IMMMM TRYYNNA BE LIKE YOUU BOOZZZ\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi --query-gpu=name --format=csv\n",
        "vram = !nvidia-smi --query-gpu=memory.total --format=csv\n",
        "vram = str(int(vram[1][:len(vram[1])-3])/1000)\n",
        "print(\"Total Vram: \"+vram+\" GB\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoEypp3b1hUr",
        "cellView": "form"
      },
      "source": [
        "#@markdown # install dependencies\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/crowsonkb/guided-diffusion\n",
        "!pip install -e ./CLIP\n",
        "!pip install -e ./guided-diffusion\n",
        "!pip install lpips datetime\n",
        "!pip install emoji --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQE-fIMnYKYK",
        "cellView": "form"
      },
      "source": [
        "#@title Download diffusion model\n",
        "#@markdown put it into the models folder where you launched jupyter from\n",
        "\n",
        "#@markdown [256x256_diffusion_uncond.pt](https://openaipublic.blob.core.windows.net/diffusion/jul-2021/256x256_diffusion_uncond.pt) <= right click open in new tab\n",
        "\n",
        "#@markdown [512x512_diffusion_uncond_finetune_008100.pt](https://v-diffusion.s3.us-west-2.amazonaws.com/512x512_diffusion_uncond_finetune_008100.pt) <= right click open in new tab\n",
        "\n",
        "import os\n",
        "!mkdir generations\n",
        "!mkdir models\n",
        "model_path = 'models'\n",
        "filename= os.listdir(os.getcwd() + '/' + model_path)\n",
        "os.startfile(os.getcwd() + '/' + model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZsjzwS0YGo6",
        "cellView": "form"
      },
      "source": [
        "\n",
        "\n",
        "#@title Choose model here:\n",
        "diffusion_model = \"256x256_diffusion_uncond\" #@param [\"256x256_diffusion_uncond\", \"512x512_diffusion_uncond_finetune_008100\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl8RO5oz1hUr",
        "cellView": "form"
      },
      "source": [
        "#@markdown # make dependecies available\n",
        "import gc\n",
        "import io\n",
        "import math\n",
        "import sys\n",
        "from IPython import display\n",
        "import lpips\n",
        "from PIL import Image, ImageOps\n",
        "import requests\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "sys.path.append('./CLIP')\n",
        "sys.path.append('./guided-diffusion')\n",
        "import clip\n",
        "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dn9Kq_tv1l2J",
        "cellView": "form"
      },
      "source": [
        "#@markdown # get more stuff ready\n",
        "#@markdown if you're interested it perlin noise \n",
        "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
        "\n",
        "def interp(t):\n",
        "    return 3 * t**2 - 2 * t ** 3\n",
        "\n",
        "def perlin(width, height, scale=10, device=None):\n",
        "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
        "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
        "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
        "    wx = 1 - interp(xs)\n",
        "    wy = 1 - interp(ys)\n",
        "    dots = 0\n",
        "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
        "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
        "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
        "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
        "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
        "\n",
        "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
        "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
        "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
        "    for i in range(1 if grayscale else 3):\n",
        "        scale = 2 ** len(octaves)\n",
        "        oct_width = width\n",
        "        oct_height = height\n",
        "        for oct in octaves:\n",
        "            p = perlin(oct_width, oct_height, scale, device)\n",
        "            out_array[i] += p * oct\n",
        "            scale //= 2\n",
        "            oct_width *= 2\n",
        "            oct_height *= 2\n",
        "    return torch.cat(out_array)\n",
        "\n",
        "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
        "    out = perlin_ms(octaves, width, height, grayscale)\n",
        "    if grayscale:\n",
        "        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n",
        "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
        "    else:\n",
        "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
        "        out = TF.resize(size=(side_y, side_x), img=out)\n",
        "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
        "\n",
        "    out = ImageOps.autocontrast(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4xKFJ9i1l2K",
        "cellView": "form"
      },
      "source": [
        "#@markdown # Load Functions\n",
        "def fetch(url_or_path):\n",
        "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
        "        r = requests.get(url_or_path)\n",
        "        r.raise_for_status()\n",
        "        fd = io.BytesIO()\n",
        "        fd.write(r.content)\n",
        "        fd.seek(0)\n",
        "        return fd\n",
        "    return open(url_or_path, 'rb')\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
        "        vals = prompt.rsplit(':', 2)\n",
        "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
        "    else:\n",
        "        vals = prompt.rsplit(':', 1)\n",
        "    vals = vals + ['', '1'][len(vals):]\n",
        "    return vals[0], float(vals[1])\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.reshape([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.reshape([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.skip_augs = skip_augs\n",
        "        self.augs = T.Compose([\n",
        "            T.RandomHorizontalFlip(p=0.5),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomAffine(degrees=15, translate=(0.1, 0.1), interpolation=TF.InterpolationMode.BILINEAR),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
        "            T.RandomGrayscale(p=0.35),\n",
        "        ])\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "\n",
        "        cutouts = []\n",
        "        for ch in range(cutn):\n",
        "            if ch > cutn - cutn//4:\n",
        "                cutout = input.clone()\n",
        "            else:\n",
        "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
        "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
        "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
        "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "\n",
        "            if not self.skip_augs:\n",
        "                cutout = self.augs(cutout)\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "            del cutout\n",
        "\n",
        "        cutouts = torch.cat(cutouts, dim=0)\n",
        "        return cutouts\n",
        "\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "def range_loss(input):\n",
        "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
        "\n",
        "def unitwise_norm(x, norm_type=2.0):\n",
        "    if x.ndim <= 1:\n",
        "        return x.norm(norm_type)\n",
        "    else:\n",
        "        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor\n",
        "        # might need special cases for other weights (possibly MHA) where this may not be true\n",
        "        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n",
        "\n",
        "def adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n",
        "    if isinstance(parameters, torch.Tensor):\n",
        "        parameters = [parameters]\n",
        "    for p in parameters:\n",
        "        if p.grad is None:\n",
        "            continue\n",
        "        p_data = p.detach()\n",
        "        g_data = p.grad.detach()\n",
        "        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n",
        "        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n",
        "        clipped_grad = g_data * (max_norm / grad_norm.clamp(min=1e-6))\n",
        "        new_grads = torch.where(grad_norm < max_norm, g_data, clipped_grad)\n",
        "        p.grad.detach().copy_(new_grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWYC7LXF1l2L",
        "cellView": "form"
      },
      "source": [
        "#@markdown # MOAR Functions!\n",
        "def regen_perlin():\n",
        "    if perlin_mode == 'color':\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
        "    elif perlin_mode == 'gray':\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "    else:\n",
        "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
        "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
        "\n",
        "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
        "    del init2\n",
        "    return init.expand(batch_size, -1, -1, -1)\n",
        "\n",
        "def do_run(x):\n",
        "    loss_values = []\n",
        " \n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "        random.seed(seed)\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # torch.backends.cudnn.deterministic = True\n",
        " \n",
        "    make_cutouts = MakeCutouts(clip_size, cutn, skip_augs=skip_augs)\n",
        "    target_embeds, weights = [], []\n",
        " \n",
        "    for prompt in text_prompts:\n",
        "        txt, weight = parse_prompt(prompt)\n",
        "        txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
        "        target_embeds.append(txt)\n",
        "        weights.append(weight)\n",
        " \n",
        "    for prompt in image_prompts:\n",
        "        path, weight = parse_prompt(prompt)\n",
        "        img = Image.open(fetch(path)).convert('RGB')\n",
        "        img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
        "        batch = make_cutouts(TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
        "        embed = clip_model.encode_image(normalize(batch)).float()\n",
        "        target_embeds.append(embed)\n",
        "        weights.extend([weight / cutn] * cutn)\n",
        " \n",
        "    target_embeds = torch.cat(target_embeds)\n",
        "    weights = torch.tensor(weights, device=device)\n",
        "    if weights.sum().abs() < 1e-3:\n",
        "        raise RuntimeError('The weights must not sum to 0.')\n",
        "    weights /= weights.sum().abs()\n",
        " \n",
        "    init = None\n",
        "    if init_image is not None:\n",
        "        init = Image.open(fetch(init_image)).convert('RGB')\n",
        "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
        "        init = TF.to_tensor(init).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device).mul(2).sub(1)\n",
        " \n",
        "    cur_t = None\n",
        "    def cond_fn(x, t, out, y=None):\n",
        "        n = x.shape[0]\n",
        "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
        "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
        "        x_in_grad = torch.zeros_like(x_in)\n",
        "\n",
        "        for i in range(cutn_batches):\n",
        "            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
        "            image_embeds = clip_model.encode_image(clip_in).float()\n",
        "            dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
        "            dists = dists.view([cutn, n, -1])\n",
        "            losses = dists.mul(weights).sum(2).mean(0)\n",
        "            loss_values.append(losses.sum().item())\n",
        "            x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n",
        "\n",
        "        tv_losses = tv_loss(x_in)\n",
        "        range_losses = range_loss(out['pred_xstart'])\n",
        "        sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
        "        loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
        "        if init is not None and init_scale:\n",
        "            init_losses = lpips_model(x_in, init)\n",
        "            loss = loss + init_losses.sum() * init_scale\n",
        "        x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
        "        grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
        "        adaptive_clip_grad([x])\n",
        "        magnitude = grad.square().mean().sqrt()\n",
        "        return grad * magnitude.clamp(max=clamp_max) / magnitude\n",
        " \n",
        "    if model_config['timestep_respacing'].startswith('ddim'):\n",
        "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
        "    else:\n",
        "        sample_fn = diffusion.p_sample_loop_progressive\n",
        " \n",
        "    original_target_embeds = target_embeds.clone()\n",
        "    for i in range(n_batches):\n",
        "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
        "\n",
        "        if fuzzy_prompt:\n",
        "            target_embeds = original_target_embeds.clone() +  torch.randn_like(target_embeds).cuda() * rand_mag\n",
        "\n",
        "        if perlin_init:\n",
        "            init = regen_perlin()\n",
        " \n",
        "        if model_config['timestep_respacing'].startswith('ddim'):\n",
        "            samples = sample_fn(\n",
        "                model,\n",
        "                (batch_size, 3, side_y, side_x),\n",
        "                clip_denoised=clip_denoised,\n",
        "                model_kwargs={},\n",
        "                cond_fn=cond_fn,\n",
        "                progress=True,\n",
        "                skip_timesteps=skip_timesteps,\n",
        "                init_image=init,\n",
        "                randomize_class=randomize_class,\n",
        "                eta=eta,\n",
        "                cond_fn_with_grad=True,\n",
        "            )\n",
        "        else:\n",
        "            samples = sample_fn(\n",
        "                model,\n",
        "                (batch_size, 3, side_y, side_x),\n",
        "                clip_denoised=clip_denoised,\n",
        "                model_kwargs={},\n",
        "                cond_fn=cond_fn,\n",
        "                progress=True,\n",
        "                skip_timesteps=skip_timesteps,\n",
        "                init_image=init,\n",
        "                randomize_class=randomize_class,\n",
        "                cond_fn_with_grad=True,\n",
        "            )\n",
        "\n",
        "        for j, sample in enumerate(samples):\n",
        "            # display.clear_output(wait=True)\n",
        "            cur_t -= 1\n",
        "            if j % display_rate == 0 or cur_t == -1:\n",
        "                for k, image in enumerate(sample['pred_xstart']):\n",
        "                    tqdm.write(f'Generated Image {x}, step {j}:')\n",
        "                    current_time = datetime.now().strftime('%H:%M:%S')\n",
        "                    filename = f'generations/generation_{x}.png'\n",
        "                    image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
        "                    image.save(filename)\n",
        "                    display.display(display.Image(filename))\n",
        " \n",
        "        plt.plot(np.array(loss_values), 'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpbody2NCR7w",
        "cellView": "form"
      },
      "source": [
        "#@markdown ### timestep_respacing = 'ddim50' # Modify this value to decrease the number of timesteps.\n",
        "timestep_respacing = '50'#@param {type: \"string\"} \n",
        "#@markdown <hr>\n",
        "\n",
        "#@markdown ### i have no clue lol\n",
        "diffusion_steps = 1000#@param {type: \"number\"}\n",
        "\n",
        "model_config = model_and_diffusion_defaults()\n",
        "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 512,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "        'use_checkpoint': True,\n",
        "    })\n",
        "elif diffusion_model == '256x256_diffusion_uncond':\n",
        "    model_config.update({\n",
        "        'attention_resolutions': '32, 16, 8',\n",
        "        'class_cond': False,\n",
        "        'diffusion_steps': diffusion_steps,\n",
        "        'rescale_timesteps': True,\n",
        "        'timestep_respacing': timestep_respacing,\n",
        "        'image_size': 256,\n",
        "        'learn_sigma': True,\n",
        "        'noise_schedule': 'linear',\n",
        "        'num_channels': 256,\n",
        "        'num_head_channels': 64,\n",
        "        'num_res_blocks': 2,\n",
        "        'resblock_updown': True,\n",
        "        'use_fp16': True,\n",
        "        'use_scale_shift_norm': True,\n",
        "        'use_checkpoint': True,\n",
        "    })\n",
        "side_x = side_y = model_config['image_size']\n",
        "\n",
        "model, diffusion = create_model_and_diffusion(**model_config)\n",
        "model.load_state_dict(torch.load(f'{model_path}/{diffusion_model}.pt', map_location='cpu'))\n",
        "model.requires_grad_(False).eval().to(device)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
        "        param.requires_grad_()\n",
        "if model_config['use_fp16']:\n",
        "    model.convert_to_fp16()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnQjGugaDZPJ",
        "cellView": "form"
      },
      "source": [
        "#@markdown # Load clip models\n",
        "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "clip_size = clip_model.visual.input_resolution\n",
        "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
        "lpips_model = lpips.LPIPS(net='vgg').to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zY-8I90LkC6"
      },
      "source": [
        "# **2)** Parameters for The Art Generator ü§ñ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0PwzFZbLfcy",
        "cellView": "form"
      },
      "source": [
        "#@markdown #### Text to be parsed and passed into CLIP for association \n",
        "text = \"alien city deviant art\" #@param {type:\"string\"}\n",
        " # @markdown <hr>\n",
        "\n",
        "#@markdown <ol> number_of_images = The number of different images you want the AI to create\n",
        "number_of_images = 5#@param {type: \"number\"}\n",
        " # @markdown <hr>\n",
        " \n",
        "\n",
        "# eta = 0.5#@param {type: \"number\"}\n",
        "text_prompts = [\n",
        "    text\n",
        "]\n",
        "\n",
        "image_prompts = [\n",
        "    # 'mona.jpg',\n",
        "]\n",
        " #@markdown #### Controls how much the image should look like the prompt.\n",
        "clip_guidance_scale = 15000#@param {type: \"number\"}\n",
        " # @markdown <hr>\n",
        " \n",
        "#@markdown #### Controls the smoothness of the final output.\n",
        "tv_scale = 2500 #@param {type: \"number\"}\n",
        " # @markdown <hr>\n",
        " \n",
        " #@markdown #### Controls how far out of range RGB values are allowed to be.\n",
        "range_scale = 100#@param {type: \"number\"}\n",
        " # @markdown <hr>\n",
        " \n",
        " #@markdown #### Controls how much saturation is allowed. From nshepperd's JAX notebook, though not sure if it's doing anything right now...\n",
        "sat_scale = 0#@param {type: \"number\"} \n",
        "# @markdown <hr>\n",
        " \n",
        " #@markdown #### Controls how many crops to take from the image. Increase for higher quality.\n",
        "cutn = 16#@param {type: \"number\"} \n",
        "# @markdown <hr>\n",
        " \n",
        " #@markdown #### Accumulate CLIP gradient from multiple batches of cuts [Can help with OOM errors / Low VRAM]\n",
        "cutn_batches = 2#@param {type: \"number\"} \n",
        "# @markdown <hr>\n",
        " \n",
        "#@markdown #### URL or local path to initial image\n",
        "init_image = None #@param {type: \"string\"}  \n",
        "# @markdown <hr>\n",
        " \n",
        "#@markdown #### This enhances the effect of the init image, a good value is 1000\n",
        "init_scale = 0#@param {type: \"number\"} \n",
        "# @markdown <hr>\n",
        " \n",
        "#@markdown #### Controls the starting point along the diffusion timesteps\n",
        "skip_timesteps = 8 #@param {type: \"number\"} \n",
        "# @markdown <hr>\n",
        " \n",
        "\n",
        "\n",
        "# Try this option for random natural-looking noise in place of an init image:\n",
        "perlin_init = True # False - Option to start with random perlin noise\n",
        "perlin_mode = 'mixed' # 'mixed' ('gray', 'color')\n",
        "if init_image is not None: # Can't combine init_image and perlin options\n",
        "  perlin_init = False\n",
        "\n",
        "skip_augs = False # False - Controls whether to skip torchvision augmentations\n",
        "randomize_class = True # True - Controls whether the imagenet class is randomly changed each iteration\n",
        "clip_denoised = False # False - Determines whether CLIP discriminates a noisy or denoised image\n",
        "clamp_max = 0.035 # 0.05 (new:0.035)\n",
        "\n",
        "fuzzy_prompt = False # False - Controls whether to add multiple noisy prompts to the prompt losses\n",
        "rand_mag = 0.05 # 0.05 - Controls the magnitude of the random noise\n",
        "eta = 0.5 # 0.5 - DDIM hyperparameter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf9hTc8YLoLx"
      },
      "source": [
        "# **3)** Generate Art üòé\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHLiO56OfwgD",
        "collapsed": true,
        "cellView": "form"
      },
      "source": [
        "#@title Run This! üèé\n",
        " #@markdown #### How often to Display Images\n",
        "display_rate = 1#@param {type: \"number\"}\n",
        " # @markdown <hr>\n",
        "\n",
        " #@markdown #### Controls how many consecutive batches of images are generated\n",
        "n_batches = 1#@param {type: \"number\"}\n",
        " # @markdown <hr>\n",
        "\n",
        " #@markdown #### Controls how many images are generated in parallel in a batch\n",
        "batch_size = 1#@param {type: \"number\"} \n",
        " # @markdown <hr>\n",
        " \n",
        "# number_of_images = 5\n",
        "\n",
        "# seed = 0\n",
        "# seed = random.randint(0, 2**32) # Choose a random seed and print it at end of run for reproduction\n",
        "for x in range(0, number_of_images):\n",
        "  # display.clear_output(wait=True)\n",
        "  seed = random.randint(0, 2**32)\n",
        "  try:\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "      do_run(x)\n",
        "  except KeyboardInterrupt:\n",
        "      pass\n",
        "  finally:\n",
        "      print('seed', seed)\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sjiHy2ygOJX",
        "collapsed": true,
        "cellView": "form"
      },
      "source": [
        "#@title Plot Generated Images! üéØ\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "images = []\n",
        "for img_path in glob.glob('/generations/*.png'):\n",
        "    images.append(mpimg.imread(img_path))\n",
        "\n",
        "plt.figure(figsize=(50,50))\n",
        "columns = 5\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "    plt.imshow(image)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#üåüüåüüåü **Beautiful Prompts you should try!** üåüüåüüåü"
      ],
      "metadata": {
        "id": "MwL-U6Nt2q5h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    the universe is a glitch by greg rutkowski\n",
        "    a space nebula rendered in Cinema4D\n",
        "    revolution of the souls, vector art\n",
        "    the rise of consciousness in the style of WPAP\n",
        "    a vaporwave dragon breathing fire by ross tran\n",
        "    \"Cognitive Transcendence\", matte painting trending on artstation\n",
        "    mirror of love, by greg rutkowski and james jean\n",
        "    falling forever through a bottomless abyss speckled with stars. painting by greg rutkowski\n",
        "    a beautiful epic fantasy painting of a giant robot\n",
        "    the fire of the mind by Ross Tran\n",
        "    the first day of the heavens! trending on artstation\n",
        "    Garden of Hesperides by ArtStation\n",
        "    a beautiful epic wondrous fantasy painting of the ocean\n",
        "    a beautiful watercolor painting of wind\n",
        "    a tropical landscape by Ivan Aivazovsky\n",
        "    a dramatic mountainous landscape by Ivan Aivazovsky\n",
        "    the aurora at night by Ivan Aivazovsky\n",
        "    a painting of a witch brewing a Halloween potion by Greg Rutkowski\n",
        "    a surreal wizards tower by Casper David\n",
        "    a beautiful epic fantasy painting of a giant robot\n",
        "    a rainy city street in the style of cyberpunk noir\n",
        "    the Tower of Babel by Thomas Kinkade\n",
        "    wings of angelic fire in the darkness, trending on artstation\n",
        "    a beautiful fantasy land forest, trending on ArtStation\n",
        "    a desert landscape by Ivan Aivazovsky\n",
        "    a heavenly cloud city by Greg Rutkowski\n",
        "    wings of angelic fire in the darkness, trending on artstation\n",
        "    "
      ],
      "metadata": {
        "id": "4k_7QguF2PPV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-\n",
        "#**Want to Super-Enhance the Generated Image with another advanced A.I.? Enhances a 500x500 image to 1760x1760 with no quality loss!!!**: \n",
        "\n",
        "-\n",
        "\n",
        "https://colab.research.google.com/drive/1clkR8lvWojym_M5cK3lTkuTJoepIOZ_-"
      ],
      "metadata": {
        "id": "ASg7_ZeOzkt3"
      }
    }
  ]
}